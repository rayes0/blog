#+author: rayes
#+hugo_base_dir: ~/pub/blog/
#+hugo_section: notes
#+hugo_front_matter_format: yaml
#+hugo_level_offset: 0
#+startup: overview customtime logdone
#+options: tex:dvisvgm
# #+macro: tex @@html:{{<tex "$1">}}@@
#+macro: tex $$1$
#+macro: dtex $$$1$$
#+macro: note @@html:{{<note "$1">}}@@

* Machine Learning
CLOSED: [2021-08-08 Sun 12:03]
:PROPERTIES:
:EXPORT_FILE_NAME: machine-learning
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :auto_summary_style false :status inprogress
:EXPORT_HUGO_LASTMOD: 2021-08-08
:END:

Some mathematical concepts and derivations that form the basis behind some simple machine learning algorithms. It can be viewed as a high level summary of introductory machine learning.

*Helpful Prior Knowledge*

- Basic linear algebra, fundamental operations with vectors and matrices, transpose and inverse
- Basic conceptual calculus, taking derivatives

** Notation
Common idiomatic notation used in machine learning and linear algebra, used throughout these notes.

- {{{tex(\theta)}}} - Vector of parameters
- {{{tex(x)}}} - Matrix of features
- {{{tex(n)}}} - Number of features
- {{{tex(m)}}} - Number of training examples
- {{{tex(y)}}} - The correct values for each set of features in the training set
- {{{tex(h_{\theta}(x))}}} - Hypothesis function
- {{{tex(J(\theta))}}} - Cost function

Given a matrix {{{tex(A)}}}:

- {{{tex(A_{ij})}}} - the entry in the {{{tex(i^{th})}}} row, {{{tex(j^{th})}}} column
- {{{tex(A^T)}}} - the transpose of {{{tex(A)}}}
- {{{tex(A^{\prime})}}} - the inverse of {{{tex(A)}}}

For our matrix of features {{{tex(x)}}}:

- {{{tex(x^{(i)})}}} - vector of the features in the {{{tex(i^{th})}}} training example
- {{{tex(x^{(i)}_{j})}}} - value of feature {{{tex(j)}}} in the {{{tex(i^{th})}}} training example

** Linear Regression
*** Basic Concepts
- Features: The numerical inputs for the algorithm from which it makes predictions.
- Parameters: The weights which we multiply to the features to produce the final output (corresponds to the prediction we make). These are the values we are trying to learn.
- Hypothesis function: Function which uses the parameters to map the input features to the final prediction. It is represented as {{{tex(h_{\theta}(x))}}}, taking the features {{{tex(x)}}} (could be a single value or a vector) as input.
- Some ways to learn {{{tex(\theta)}}} values:
  - Gradient Descent:
    - Cost function: Function that returns the error of the hypothesis function and the actual correct values in the training set. It is represented as {{{tex(J(\theta))}}}, taking our parameters used to make the prediction ({{{tex(\theta)}}}) as input.
    - We try to find values of {{{tex(\theta)}}} which minimize the cost function, because the lower the cost function, the lower our error is. We do this by finding the global minimum.
  - Normal Equation: An equation to calculate the minimum without the need for gradient descent. We will talk about this later.
*** How it works
Linear regression fits a model to a straight line dataset, therefore our hypothesis function for univariate (one feature) linear regression is:

$$h_{\theta}(x) = \theta_0 + \theta_1x$$

This is a basic 2D straight line, where {{{tex(x)}}} is our feature and we are trying to learn the bias parameter {{{tex(\theta_0)}}} and the weight parameter {{{tex(\theta_1)}}}. We only have a single feature parameter because we only have one feature.

If we do the same for multiple features, we will get a linear multi-dimensional equation:

$$h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n$$

Where {{{tex(n)}}} is the number of features. Each feature (the {{{tex(x)}}} terms) has a weight parameter ({{{tex(\theta_1)}}} through {{{tex(\theta_n)}}}), and each of the individual bias terms are collected into one term {{{tex(\theta_0)}}}. Notice how the input {{{tex(x)}}} is no longer a single value, and is instead a collection of values {{{tex(x_1\, x_2\, x_3 \cdots x_n)}}}, which we can represent as a column vector. We can do the same thing with our {{{tex(\theta)}}} values:

$$x = \begin{bmatrix} x_0 = 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \ \ \ \ \ \ \theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n \end{bmatrix}$$

Notice how we have added an extra {{{tex(x_0)}}} term into the start of the {{{tex(x)}}} vector, and we set it equal to 1. This corresponds to the bias term {{{tex(\theta_0)}}}, which we used in the hypothesis equations. We do this because {{{tex(\theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n= \theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_n x_n)}}} if {{{tex(x_0 = 1)}}}. This also matches the dimensions of both vectors, enabling us to do operations such as multiplication with them.

With our two matrices, we can write out a vectorized version of the hypothesis function as {{{tex(\theta^T x)}}}, which we can see is equivalent to our original equation:

$$h_{\theta}(x) = \theta^T x = \begin{bmatrix} \theta_0 & \theta_1 & \theta_2 & \cdots & \theta_n \end{bmatrix} \times \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$$

*** Gradient Descent
One way to learn the values of {{{tex(\theta)}}} is gradient descent. In order to implement this, we need a cost function which calculates the error of our hypothesis function above. There are a variety of cost functions that could be used, but the typical one for simple regression is a variation on the average of the [[https://en.wikipedia.org/wiki/Variance][squared error]]:

# $$J(\theta) = \dfrac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2"$$

$$J(\theta) = \dfrac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2$$

Recall that {{{tex(m)}}} represents the number of training examples we have, and {{{tex(y)}}} represents the actual correct predictions for each set of {{{tex(x)}}} features in our training set. What this function does is for each of our training sets, take the value of the hypothesis for that set ({{{tex(h_{\theta}(x^{(i)}))}}}), and calculate the difference between it and the corresponding actual value, then square that difference. This guarantees a positive value. We then sum up each one of these squared positive values, then divides by {{{tex(2m)}}}, a slight variation on calculating the squared mean error (which would just be dividing by {{{tex(m)}}} only). The reason we also divide by 2 is because it makes the derivative nicer, as the term inside the summation is squared. When we derive this, will end up with a coefficient of 2 in front, which will nicely cancel with the 2 in the denominator.

The actual gradient descent step comes from finding values of {{{tex(\theta)}}} that minimize this function the most, in other words, the global minimum. At the minimum point, the derivative (in this case the partial derivative) of the cost function in terms of {{{tex(\theta)}}} will be 0. We can calculate the derivative as follows:
\\
\\
\begin{align*}
\dfrac{\delta}{\delta\theta} J(\theta) &= \dfrac{1}{2m} \cdot \dfrac{\delta}{\delta\theta} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2  &\text{(Note: } m \text{ is a constant)} \\ 
&= \dfrac{1}{2m} \cdot \sum_{i=1}^n \dfrac{\delta}{\delta \theta} (\theta^T x^{(i)} - y^{(i)})^2  &(h_{\theta}(x^{(i)}) \text{ is substituted for } \theta^T x^{(i)}) \\
&= \dfrac{1}{2m} \cdot \sum_{i=1}^m \:2(\theta^Tx^{(i)} - y ^{(i)}) \cdot x^{(i)} &\text{(Note: } y^{(i)} \text{ is a constant)} \\
&= \dfrac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}  &\text{(Simplify and substitute back } h_{\theta}(x^{(i)}))
\end{align*}

One way to get to the minimum is to repeatedly subtract the value of the derivative from the old {{{tex(\theta)}}} value. By doing this, when the derivative is positive (indicating we are to the right of the minimum), {{{tex(\theta)}}} will be lowered (move to the left), when the derivative is negative (indicating we are to the left of the minimum), {{{tex(\theta)}}} will be raised (move to the right). Thus, with many iterations of this, we will eventually approach the minimum. Here is the mathematical representation (the {{{tex(:=)}}} is used to show that we are updating the value, rather than as an equality operator):

\begin{align*} & \text{For } j = 0, \cdots, n \\ & \text{repeat until convergence \{} \\ & \qquad \theta_j := \theta_j - \alpha \dfrac{\delta}{\delta \theta_j} J(\theta) \\ &\}\end{align*}

Substituting the derivative we took above. {{{tex(x^{(i)})}}} is replaced with {{{tex(x^{(i)}_j)}}} because when dealing with multiple features, we mean to say the feature set for the specific training example:

\begin{align*} & \text{For } j = 0, \cdots, n \\ & \text{repeat until convergence \{} \\ & \qquad \theta_j := \theta_j - \dfrac{\alpha}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_j \\ &\}\end{align*}

We have added a new variable: {{{tex(\alpha)}}}. This is called the learning rate, and as you can probably guess from the equation, it corresponds to the size of step we take with each iteration. A large {{{tex(\alpha)}}} value will lead to subtracting or adding larger values to {{{tex(\theta_j)}}} each time. Too small of a learning rate will lead to gradient descent taking too long to converge, because we are taking very small steps each time. Too large of a learning rate can cause our algorithm to never converge because it will overshoot the minimum each time.

One important point is that we are repeating this step for multiple variables. If we were to write it out fully, assuming we have 50 features (meaning that {{{tex(x \in \mathbb{R}^{51})}}} and {{{tex(\theta \in \mathbb{R}^{51})}}}):

\begin{align*} & \text{repeat until convergence \{} \\ & \qquad \theta_0 := \theta_0 - \dfrac{\alpha}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_0 \\ & \qquad \theta_1 := \theta_1 - \dfrac{\alpha}{m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)})x^{(i)}_1 \\ & \qquad \theta_2 := \theta_2 - \dfrac{\alpha}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_2 \\ & \qquad \qquad \vdots \\ & \qquad \theta_{51} := \theta_{51} - \frac{\alpha}{m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)})x^{(i)}_{51} \\ &\}\end{align*}

Because our {{{tex(h_{\theta}(x^{(i)}))}}} is dependent on the values of the parameter vector {{{tex(\theta)}}}, we need to make sure we are updating our values simultaneously after we are done with the computations. Consider the following incorrect psuedocode for a single gradient descent step on a three parameters:

#+begin_src
# assume:
#   theta_0 is the bias term
#   theta_1 is the 1st parameter, theta_2 is the second parameter, ... etc.
#   alpha is the learning rate
#   dcost_1, dcost_2, ... etc. is the partial derivative of the cost function for each respective theta

theta_0 = theta_0 - ((alpha / m) * dcost_0)
theta_1 = theta_1 - ((alpha / m) * dcost_1)
theta_2 = theta_2 - ((alpha / m) * dcost_2)
#+end_src

This is wrong because we are updating the values before we are finished using all of them yet! Here is a correct implementation, where we update the {{{tex(\theta)}}} values simultaneously after the computation:

#+begin_src
temp0 = theta_0 - ((alpha / m) * dcost_0)
temp1 = theta_1 - ((alpha / m) * dcost_1)
temp2 = theta_2 - ((alpha / m) * dcost_2)

theta_0 = temp0
theta_1 = temp1
theta_2 = temp2
#+end_src

* Modern Asian Pop Music Observations                          :@Music:anime:
CLOSED: [2022-03-17 Thu 14:40]
:PROPERTIES:
:EXPORT_FILE_NAME: anime-pop
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :status inprogress
:END:
Some aggregated observations of common harmonic tendencies and chord progressions that give the modernistic Asian pop sound found in a lot of contemporary music from the East Asia region, in particular focusing on anime and film music. Some good examples of this type of sound include the music of film composers like Hayao Miyazaki, Joe Hisaishi, and Ryuichi Sakamoto, almost every anime opening or ending, some J-pop and C-pop artists, in particular mandopop artists like Jay Chou, Mayday, JJ Lin, contemporary Asian instrumental music (Yiruma in particular), and Western artists that also use this sound (Porter Robinson, even contemporary or neoclassical artists like Ludovico Einaudi and Maurice Ravel in certain sections of their music).

{{{note(These are based on my observations only. I am not a musical historian nor a music expert\, and I only play music as a side hobby.)}}}

** Western Pop Origins
Since East Asian pop is descended from Western pop music, it makes sense to first take a look into the influential progressions that shaped Western pop. Namely, the following types of chord changes and their variants:

- Main harmonic progression :: Some mix of I, vi, IV, and V, notable examples include:
  - [[https://en.wikipedia.org/wiki/I%E2%80%93V%E2%80%93vi%E2%80%93IV_progression][I-V-vi-IV]]
  - [[https://en.wikipedia.org/wiki/%2750s_progression][I-vi-IV-V]]
- Main cadences/resolutions/"special" chords :: The classic resolutions that are most common are obviously IV-V-I and ii-V-I. Other fairly common ones include:
  - The V-vi deceptive cadence. When combined with the subdominant, it becomes IV-V-vi (or what appears to be more common, the descending version vi-V-IV), and vi-V-IV
    - An interesting, recessed alternative from classical music is what I like to call the "plagal deceptive cadence" (IV-vi). From what I see, it typically pops up in the middle of progressions, but it can be used as a cadence.
  - [[https://en.wikipedia.org/wiki/Mixolydian_mode][Mixolydian progressions]], basically progressions containing some form of a ♭VII. From my understanding, this came from jazz and the concept of [[https://en.wikipedia.org/wiki/Borrowed_chord][borrowed chords]]. The use of a occasional ♭VII with the main I, vi, IV, and V chords gives a distinctly "modern pop" sound and is very common. See [[https://en.wikipedia.org/wiki/%E2%99%ADVII%E2%80%93V7_cadence][♭VII–V7]] and [[https://en.wikipedia.org/wiki/Backdoor_progression][backdoor progressions]] for examples.
  - Progessions around the circle of fifths/fourths (I-IV-vii^{o}-iii-vi-ii-V-I), or [[https://en.wikipedia.org/wiki/Pachelbel%27s_Canon][similar variants]]. Commonly used in the middle of progressions for smooth modulations between keys.

** Asian Pop Chords. Why IV-V-iii-vi works
It seems a trend for Asian style pop to either start on a subdominant chord (IV), or to have it at least on a strong beat. In my view, the reason the IV is an acceptable starting chord and why it's used over other chords is because IV contains the major tonic note, as well as tonic note of the relative minor, hinting at both of these. Because the first chord usually establishes the general feel and reference point for the rest of the chords, it makes sense to choose IV over chords like V for this purpose.
** IV-V-vi and a few (of many) variants
The IV-V-vi is a very common chord progression in not just pop music, and is just a deceptive cadence (V-vi) with a subdominant IV to set it up. The reason it works, especially for pop, is that it's easy to voice lead the bass (it's just whole tones), which is a large part of what makes or breaks a progression. Here are a few of the more popular variants:

- [[https://en.wikipedia.org/wiki/IV%E2%96%B37%E2%80%93V7%E2%80%93iii7%E2%80%93vi_progression][IV-V-iii-vi]]. The "royal road progression". AKA the progression that's used in pretty much every anime opening. This is pretty much the same thing as a IV-V-vi, but since 4/4 music is often in four bar phrases, having four chords means we don't need to repeat one of chords. iii is the same thing as a V/vi (without a raised leading tone) and resolves nicely to the vi.
  - IV-V-III^{#3}-vi. You may have deduced that if iii is the same as V/vi (the dominant chord of the relative minor), we can raise the seventh making it a III^{#3}. This creates a more atonalic sound because we have more or less migrated to the minor key. The III^{#3} is often used in transitions between section changes, and is so common that I might even venture to call it a norm in this genre.
- IV-V-vi-I.
- IV-V-I-vi.
- IV-I-V-vi. Same progression as the famous [[https://en.wikipedia.org/wiki/I%E2%80%93V%E2%80%93vi%E2%80%93IV%5Fprogression][I-V-vi-IV]] that we mentioned above for Western music, only rotated so that the starting chord is on the IV.
- vi-V-IV-(I). Instead of IV-V-vi, we can reverse it and start on the relative minor.
# Examples: intro of 'Homura' by LiSA.

** Use of iii
In the IV-V-iii-vi progression, the iii assumes the role of V/vi. The iii chord can also be used to lead into the IV, being only one semitone away, which (similarly as before) works because of the voice leading in the bass. Some chord changes in pop music break the rules of functional harmony, however, it will sound fine as long as the voice leading in the bass and melody is smooth.

Some examples of progressions with iii leading to IV:
- IV-V-vi-iii
- I-iii-IV-V
# Examples: Hitorigoto

** COMMENT Modulation

* TODO Structure and Interpretation of Computer Programs
* TODO Audio impressions
Few audio impressions.
** Moondrop Aria
** Hifiman he400se
- Build: Could be better, crackling when adjusting coming from headband. Headband okay, but I hear it's replaceable
- Comfort:
- Sound: It's okay overall, treble timbre is very off without eq, and even with eq it is still mechanical-sounding
** 
* Audio                                                              :@Music:
CLOSED: <2023-04-16 Sun>
:PROPERTIES:
:EXPORT_FILE_NAME: audio
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :status draft
:END:

A rigorous crash course to essential basics of audio science for critical evaluation of audio gear. The aim is to cover everything needed for a proper understanding of the effects of audio equipment in both objective and subjective audio playback.

** Introduction
Though it may seem like sound reproduction is highly visceral, audio is a very mature field and there is much research that has been done in domain from which it is possible to gain at least some general insight into how gear precisely shapes sound. However, it is a topic that can get complex very quickly, and compounded with the copious amounts of variables in play means that oftentimes, a statement is correct only within a specific context (sometimes a highly niche one). Any small amount of vagueness means that individuals can easily misinterpret statements, which is why a solid understanding of the fundamentals of the field is very useful before forming opinions on gear.

Due to the inherent subjectivity of audio, there are many enthusiasts (especially online) who have strong opinions, whom are also

The primary purpose of this text is to inform.

** Signal Processing
*** What is Sound
Sound is some vibration which propagates through some medium as an acoustic wave. An acoustic wave is a type of mechanical wave as it transmits through the mechanical movements of particles. However, when people talk about sound, they can also mean the receiving of these waves by the ear. I will not get into the question of 'if a tree falls in a forest but no one is there to hear it, does it make a sound?', which arises from the disparity between these two definitions and is a useless debate. I'm sure if you put a microphone in the forest and recorded the tree falling, nobody playing it back would expect to hear anything different.

It will be obvious from context when someone says 'sound', whether they are talking about the propagation of an acoustic wave, or the reception of such a propagation by the sensory system, or whether it even matters which one.

*** Analog and Digital
**** Sampling Theory
Analog and digital describes the transmission or storage method of a signal such as a sound wave. An analog signal is continuous whereas a digital signal is discrete. As suggested by the name, an analog sound signal represents the entire waveform analogously (eg: the surface of a vinyl record represents a direct correspondence to the sound signal). On the other hand, a digital signal is sampled at finite number of disparate intervals. This is needed because our digital systems cannot process continuous analog signals. Often, the digital signal is sampled from an analog one (a process called /analog-to-digital conversion/) using a standard called PCM (pulse-code modulation). It works as follows:
1. Sample the amplitude of a continuous analog signal at specific time intervals. The time interval used is called the /sample rate/. Typical sample rates for audio are 44.1 kHz (44100 samples per second) and 48 kHz (48000 samples per second).
2. For each sample, perform /quantization/, just a fancy term for using some function to round or truncate to a specific value. This transforms the continuous set of possible sample values read from the analog signal into a discrete set of 'levels', enabling the ability to store the value without requiring infinite precision. The number of possible levels is dictated by the /word-length/, which represents how many values is allocated for each sample. Common word-lengths for audio are powers of two, and are referred to as /bit-depth/: eg: 8-bit, 16-bit, 24-bit, 32-bit.


Therefore, digital audio is an approximation of a continuous analog signal using discrete samples and finite bit-depth. This means that in theory, the higher the sample rate and the higher the bit depth, the closer the digital signal will match the analog one. However, we needn't use infinite sample rates and bit depths because we can use techniques to interpolate the discrete signal to gain back some level of continuity:

- Sampling theorem (Nyquist-Shannon) :: Suppose we sample a continuous audio signal with some sample rate and do not perform quantization (we don't round the values for each sample). Then we can perfectly reconstruct the continuous signal for frequencies up to half the sample rate. This maximum frequency we can interpolate to is called the /Nyquist frequency/.
  - In practise, because we do quantize the signal: A continuous signal with some maximum frequency can be perfectly reconstructed /to the accuracy of the quantization/ if the maximum frequency is less than or equal to the Nyquist frequency.


Here is a drastically simplified example to provide some intuition:
- Suppose we sample the green sine wave at a certain sample rate as shown (red points are our samples):

#+begin_src gnuplot :file ../static/img/audio/nyquist.png :cache yes
reset session
set term png size 600,250
set xzeroaxis ls 1 lc rgb "gray" lw 1
set yzeroaxis ls 1 lc rgb "grey" lw 1
unset border
unset xtics
unset ytics
set xrange [0:6.3]
set samples 1000
set term png transparent truecolor
set arrow from 2,0 to 2,0.909 nohead lw 5 lc rgb "grey"
set arrow from 4,0 to 4,-0.757 nohead lw 5 lc rgb "grey"
set arrow from 6,0 to 6,-0.279 nohead lw 5 lc rgb "grey"
f(x) = sin(x)
g(x) = sin(4.14*x)
plot g(x) with lines notitle lw 2 ,f(x) with lines notitle lw 2, '-' w p pt 7 ps 3 lc rgb "brown" notitle
0 0
2 0.909
4 -0.757
6 -0.279
e
#+end_src

#+RESULTS[2e96aa1d26bda6c7171c1bd1a6a3611df3513b90]:
[[file:../static/img/audio/nyquist.png]]

- We can see that at this sample rate, the purple waveform (which has a frequency more than twice the sample rate) will give the exact same samples as the green wave. Always using a sample rate at least twice the highest frequency we need to capture solves this problem.


Therefore, we can accurately interpolate (to the accuracy of our quantization) any sampled digital signal to its initial analog version as long as the maximum frequency does not exceed the Nyquist frequency. In terms of audio, human hearing is commonly cited to extend up to 20 kHz (we will get into this more later). Therefore, to capture frequencies that cover the entire range of hearing, a minimum sample rate of 40 kHz is required.

**** Bit-Depth and Dithering
The Nyquist-Shannon Sampling theory means that we are able to perfectly reconstruct signals up to half the sample rate. However, this is limited by how accurately we can store the values of the samples. In practise we must quantize the signal by rounding or truncating to a finite level of precision, because the nature of our digital systems are not capable of storing values of infinite precision.

** Sound Reproduction

** Philosophy of Audio
*** Psychoacoustics

